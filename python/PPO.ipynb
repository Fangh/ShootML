{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 4000000 # Set maximum number of steps to run environment.\n",
    "run_path = \"mousegame2\" # The sub-directory name for model and summary statistics\n",
    "load_model = True # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 10000 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"mousegame2\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 5000 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 512 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'learning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 4\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 2\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-2000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-2000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2010000. Mean Reward: 0.7590460878885231. Std of Reward: 2.330375644435469.\n",
      "Step: 2020000. Mean Reward: 1.0625059382422677. Std of Reward: 2.835183055849889.\n",
      "Step: 2030000. Mean Reward: 1.4984731774415225. Std of Reward: 3.5965888632815766.\n",
      "Step: 2040000. Mean Reward: 1.031648998822131. Std of Reward: 2.7177402818477208.\n",
      "Step: 2050000. Mean Reward: 1.3526832641770234. Std of Reward: 2.933627703024844.\n",
      "Saved Model\n",
      "Step: 2060000. Mean Reward: 1.439150417827281. Std of Reward: 3.2033557834596462.\n",
      "Step: 2070000. Mean Reward: 1.715378787878768. Std of Reward: 3.6239082379560172.\n",
      "Step: 2080000. Mean Reward: 1.8124336973478719. Std of Reward: 3.578892577353645.\n",
      "Step: 2090000. Mean Reward: 1.420124137931018. Std of Reward: 3.1107537813266117.\n",
      "Step: 2100000. Mean Reward: 1.3666897506925053. Std of Reward: 2.9093550009364075.\n",
      "Saved Model\n",
      "Step: 2110000. Mean Reward: 1.9821943048575976. Std of Reward: 3.6651635472706983.\n",
      "Step: 2120000. Mean Reward: 1.9272861842105022. Std of Reward: 3.649595918107466.\n",
      "Step: 2130000. Mean Reward: 2.0562522522522264. Std of Reward: 3.4531788735547546.\n",
      "Step: 2140000. Mean Reward: 1.744362519201207. Std of Reward: 3.263425257193147.\n",
      "Step: 2150000. Mean Reward: 2.2523175182481463. Std of Reward: 3.8926193395372533.\n",
      "Saved Model\n",
      "Step: 2160000. Mean Reward: 2.904258872651321. Std of Reward: 4.507584726502226.\n",
      "Step: 2170000. Mean Reward: 2.763293413173618. Std of Reward: 4.3783855941333245.\n",
      "Step: 2180000. Mean Reward: 1.3453823953823796. Std of Reward: 2.3181747633176846.\n",
      "Step: 2190000. Mean Reward: 1.791534653465324. Std of Reward: 2.908803660260936.\n",
      "Step: 2200000. Mean Reward: 1.398162083936307. Std of Reward: 2.4742004119521748.\n",
      "Saved Model\n",
      "Step: 2210000. Mean Reward: 2.3355555555555267. Std of Reward: 3.686439759621655.\n",
      "Step: 2220000. Mean Reward: 3.76592233009704. Std of Reward: 5.099978904752275.\n",
      "Step: 2230000. Mean Reward: 3.07414316702816. Std of Reward: 4.548184683889295.\n",
      "Step: 2240000. Mean Reward: 3.853776041666617. Std of Reward: 5.009246976618215.\n",
      "Step: 2250000. Mean Reward: 4.512105263157836. Std of Reward: 5.261457935989577.\n",
      "Saved Model\n",
      "Step: 2260000. Mean Reward: 3.98210796915162. Std of Reward: 5.015828691131613.\n",
      "Step: 2270000. Mean Reward: 4.297617728531799. Std of Reward: 5.308961104490454.\n",
      "Step: 2280000. Mean Reward: 5.9608496732025404. Std of Reward: 6.153631952445576.\n",
      "Step: 2290000. Mean Reward: 5.880032467532396. Std of Reward: 6.091891006732314.\n",
      "Step: 2300000. Mean Reward: 5.684517133956318. Std of Reward: 6.090619658113535.\n",
      "Saved Model\n",
      "Step: 2310000. Mean Reward: 5.400153846153781. Std of Reward: 6.098967401203739.\n",
      "Step: 2320000. Mean Reward: 6.315860927152241. Std of Reward: 6.414531806651097.\n",
      "Step: 2330000. Mean Reward: 6.337993197278834. Std of Reward: 6.373885367672131.\n",
      "Step: 2340000. Mean Reward: 5.36910179640712. Std of Reward: 6.189374740009797.\n",
      "Step: 2350000. Mean Reward: 5.028499999999942. Std of Reward: 6.156061238504807.\n",
      "Saved Model\n",
      "Step: 2360000. Mean Reward: 5.068631284916141. Std of Reward: 6.347574698355198.\n",
      "Step: 2370000. Mean Reward: 6.0727760252365215. Std of Reward: 6.659742598140983.\n",
      "Step: 2380000. Mean Reward: 5.628119402985008. Std of Reward: 6.597975382639817.\n",
      "Step: 2390000. Mean Reward: 5.047978142076443. Std of Reward: 6.499777487537509.\n",
      "Step: 2400000. Mean Reward: 4.962827225130832. Std of Reward: 6.554914176417586.\n",
      "Saved Model\n",
      "Step: 2410000. Mean Reward: 4.504419191919139. Std of Reward: 6.188497999591171.\n",
      "Step: 2420000. Mean Reward: 3.098004115226299. Std of Reward: 5.220943689825442.\n",
      "Step: 2430000. Mean Reward: 2.6088930581613177. Std of Reward: 4.616297714707571.\n",
      "Step: 2440000. Mean Reward: 2.713020637898654. Std of Reward: 5.034021893994603.\n",
      "Step: 2450000. Mean Reward: 4.1836626506023595. Std of Reward: 6.031918201318663.\n",
      "Saved Model\n",
      "Step: 2460000. Mean Reward: 3.9559712230215354. Std of Reward: 5.842222490206135.\n",
      "Step: 2470000. Mean Reward: 3.431276595744638. Std of Reward: 5.5064375930402845.\n",
      "Step: 2480000. Mean Reward: 4.104428223844232. Std of Reward: 5.931464400006601.\n",
      "Step: 2490000. Mean Reward: 2.708261682242956. Std of Reward: 4.920505346618502.\n",
      "Step: 2500000. Mean Reward: 2.087852459016367. Std of Reward: 4.350290038704793.\n",
      "Saved Model\n",
      "Step: 2510000. Mean Reward: 1.3576546391752389. Std of Reward: 3.6802399930573504.\n",
      "Step: 2520000. Mean Reward: 1.713530259365972. Std of Reward: 3.9458156531181676.\n",
      "Step: 2530000. Mean Reward: 1.1107547169811176. Std of Reward: 3.270974394122613.\n",
      "Step: 2540000. Mean Reward: 0.7534493670885976. Std of Reward: 2.3863889312939777.\n",
      "Step: 2550000. Mean Reward: 2.5986303939962148. Std of Reward: 4.727612731515584.\n",
      "Saved Model\n",
      "Step: 2560000. Mean Reward: 4.097178217821732. Std of Reward: 5.637824518807882.\n",
      "Step: 2570000. Mean Reward: 3.7940281030444503. Std of Reward: 5.606894441062197.\n",
      "Step: 2580000. Mean Reward: 3.0917864476385652. Std of Reward: 5.100779009190264.\n",
      "Step: 2590000. Mean Reward: 2.502072243345976. Std of Reward: 4.206433521186861.\n",
      "Step: 2600000. Mean Reward: 3.411321184510208. Std of Reward: 5.122703459833155.\n",
      "Saved Model\n",
      "Step: 2610000. Mean Reward: 3.9401707317072687. Std of Reward: 5.487109658998476.\n",
      "Step: 2620000. Mean Reward: 4.520215633423126. Std of Reward: 5.722319705385598.\n",
      "Step: 2630000. Mean Reward: 4.585403225806395. Std of Reward: 5.7804608402427755.\n",
      "Step: 2640000. Mean Reward: 4.413878627968284. Std of Reward: 5.678622737631887.\n",
      "Step: 2650000. Mean Reward: 5.594325153374166. Std of Reward: 6.244228795955316.\n",
      "Saved Model\n",
      "Step: 2660000. Mean Reward: 6.268762214983641. Std of Reward: 6.689284622517332.\n",
      "Step: 2670000. Mean Reward: 5.881406249999929. Std of Reward: 6.347669076910045.\n",
      "Step: 2680000. Mean Reward: 4.6282880434782045. Std of Reward: 5.711678497078307.\n",
      "Step: 2690000. Mean Reward: 4.4024146981626755. Std of Reward: 5.670062787747686.\n",
      "Step: 2700000. Mean Reward: 5.298960244648255. Std of Reward: 6.130640408970886.\n",
      "Saved Model\n",
      "Step: 2710000. Mean Reward: 5.267694524495614. Std of Reward: 6.127677690435864.\n",
      "Step: 2720000. Mean Reward: 5.035942857142797. Std of Reward: 6.091748761083504.\n",
      "Step: 2730000. Mean Reward: 5.110838150288956. Std of Reward: 6.088781605233642.\n",
      "Step: 2740000. Mean Reward: 4.838854748603292. Std of Reward: 5.953643603045581.\n",
      "Step: 2750000. Mean Reward: 5.513622291021605. Std of Reward: 6.08737002948818.\n",
      "Saved Model\n",
      "Step: 2760000. Mean Reward: 4.1670076726342185. Std of Reward: 5.46452469812435.\n",
      "Step: 2770000. Mean Reward: 3.358803611738107. Std of Reward: 4.88741477405189.\n",
      "Step: 2780000. Mean Reward: 3.030191082802509. Std of Reward: 4.550612399655073.\n",
      "Step: 2790000. Mean Reward: 2.3283615819208734. Std of Reward: 3.7474536425878284.\n",
      "Step: 2800000. Mean Reward: 2.436763565891442. Std of Reward: 3.7362965888580772.\n",
      "Saved Model\n",
      "Step: 2810000. Mean Reward: 2.4347637795275263. Std of Reward: 3.669915967963853.\n",
      "Step: 2820000. Mean Reward: 2.46727095516566. Std of Reward: 3.861232766648475.\n",
      "Step: 2830000. Mean Reward: 1.8802814569536184. Std of Reward: 3.25894252446025.\n",
      "Step: 2840000. Mean Reward: 2.6369762845849465. Std of Reward: 4.2188017128940505.\n",
      "Step: 2850000. Mean Reward: 2.942831460674116. Std of Reward: 4.048859024288245.\n",
      "Saved Model\n",
      "Step: 2860000. Mean Reward: 3.1574545454545033. Std of Reward: 4.436616377860162.\n",
      "Step: 2870000. Mean Reward: 3.7087621359222833. Std of Reward: 4.994942896135359.\n",
      "Step: 2880000. Mean Reward: 3.779421686746942. Std of Reward: 5.259275652449175.\n",
      "Step: 2890000. Mean Reward: 4.644679665738104. Std of Reward: 5.530950680184478.\n",
      "Step: 2900000. Mean Reward: 3.83758354755779. Std of Reward: 5.002875093315582.\n",
      "Saved Model\n",
      "Step: 2910000. Mean Reward: 5.119845679012279. Std of Reward: 5.519668896706643.\n",
      "Step: 2920000. Mean Reward: 4.198514588859362. Std of Reward: 5.189809584464161.\n",
      "Step: 2930000. Mean Reward: 3.9416358839049606. Std of Reward: 5.001987411286589.\n",
      "Step: 2940000. Mean Reward: 4.411820652173858. Std of Reward: 5.406559190747019.\n",
      "Step: 2950000. Mean Reward: 4.675999999999938. Std of Reward: 5.301847607954263.\n",
      "Saved Model\n",
      "Step: 2960000. Mean Reward: 4.35374316939885. Std of Reward: 5.301469270479708.\n",
      "Step: 2970000. Mean Reward: 3.9056847545219116. Std of Reward: 4.880860392853038.\n",
      "Step: 2980000. Mean Reward: 4.2986720867208135. Std of Reward: 5.323519489422465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2990000. Mean Reward: 4.631902017291004. Std of Reward: 5.284231162240113.\n",
      "Step: 3000000. Mean Reward: 5.001509433962197. Std of Reward: 5.286189159954077.\n",
      "Saved Model\n",
      "Step: 3010000. Mean Reward: 6.595567375886442. Std of Reward: 6.158828397615511.\n",
      "Step: 3020000. Mean Reward: 6.416561403508691. Std of Reward: 6.038114620976761.\n",
      "Step: 3030000. Mean Reward: 5.428620689655102. Std of Reward: 5.811981892667911.\n",
      "Step: 3040000. Mean Reward: 5.585324675324603. Std of Reward: 5.745057974324664.\n",
      "Step: 3050000. Mean Reward: 5.238132530120415. Std of Reward: 5.967453656213968.\n",
      "Saved Model\n",
      "Step: 3060000. Mean Reward: 4.061653543307034. Std of Reward: 4.910829015833206.\n",
      "Step: 3070000. Mean Reward: 3.183187066974554. Std of Reward: 4.3848442007412896.\n",
      "Step: 3080000. Mean Reward: 3.791091811414344. Std of Reward: 5.101952460304075.\n",
      "Step: 3090000. Mean Reward: 4.92497058823523. Std of Reward: 5.5195132190805385.\n",
      "Step: 3100000. Mean Reward: 5.218079268292617. Std of Reward: 5.793106734737175.\n",
      "Saved Model\n",
      "Step: 3110000. Mean Reward: 5.673815789473613. Std of Reward: 5.791372545957107.\n",
      "Step: 3120000. Mean Reward: 6.010067567567491. Std of Reward: 5.887631733376864.\n",
      "Step: 3130000. Mean Reward: 6.278127090300926. Std of Reward: 6.220556855771622.\n",
      "Step: 3140000. Mean Reward: 6.631709090909008. Std of Reward: 6.187523545366267.\n",
      "Step: 3150000. Mean Reward: 7.414627450980299. Std of Reward: 6.229828606715395.\n",
      "Saved Model\n",
      "Step: 3160000. Mean Reward: 8.923711790392904. Std of Reward: 6.352420968534425.\n",
      "Step: 3170000. Mean Reward: 9.454911504424668. Std of Reward: 6.459636708677726.\n",
      "Step: 3180000. Mean Reward: 8.384024896265457. Std of Reward: 6.3338779544042385.\n",
      "Step: 3190000. Mean Reward: 7.805393700787308. Std of Reward: 6.524794174395624.\n",
      "Step: 3200000. Mean Reward: 7.267752808988676. Std of Reward: 6.5087136787256.\n",
      "Saved Model\n",
      "Step: 3210000. Mean Reward: 6.803597122302074. Std of Reward: 6.314356202730002.\n",
      "Step: 3220000. Mean Reward: 6.79553956834524. Std of Reward: 6.241267761442078.\n",
      "Step: 3230000. Mean Reward: 6.524628975264937. Std of Reward: 6.212081195342999.\n",
      "Step: 3240000. Mean Reward: 6.1874489795917595. Std of Reward: 6.088297905281122.\n",
      "Step: 3250000. Mean Reward: 7.2819157088121695. Std of Reward: 6.259422825455766.\n",
      "Saved Model\n",
      "Step: 3260000. Mean Reward: 6.916470588235208. Std of Reward: 6.239269851396543.\n",
      "Step: 3270000. Mean Reward: 7.114507575757488. Std of Reward: 6.3142536375997524.\n",
      "Step: 3280000. Mean Reward: 6.6624028268550415. Std of Reward: 6.169127217214926.\n",
      "Step: 3290000. Mean Reward: 5.98842809364541. Std of Reward: 6.008643762967373.\n",
      "Step: 3300000. Mean Reward: 5.1167173252278975. Std of Reward: 5.693359982177445.\n",
      "Saved Model\n",
      "Step: 3310000. Mean Reward: 6.0285620915031926. Std of Reward: 6.123839990812291.\n",
      "Step: 3320000. Mean Reward: 6.149094076654974. Std of Reward: 6.0568585722854635.\n",
      "Step: 3330000. Mean Reward: 5.502948717948646. Std of Reward: 5.7367656748054285.\n",
      "Step: 3340000. Mean Reward: 5.835639344262222. Std of Reward: 5.967740297915891.\n",
      "Step: 3350000. Mean Reward: 6.500706713780838. Std of Reward: 6.1827111789049445.\n",
      "Saved Model\n",
      "Step: 3360000. Mean Reward: 7.285381679389223. Std of Reward: 6.302109701233471.\n",
      "Step: 3370000. Mean Reward: 6.25578231292509. Std of Reward: 6.077291614492223.\n",
      "Step: 3380000. Mean Reward: 6.118019801980123. Std of Reward: 6.158552992116145.\n",
      "Step: 3390000. Mean Reward: 6.042680412371059. Std of Reward: 6.028157329391567.\n",
      "Step: 3400000. Mean Reward: 6.599330985915412. Std of Reward: 6.107123862864005.\n",
      "Saved Model\n",
      "Step: 3410000. Mean Reward: 5.1210869565216735. Std of Reward: 5.632630785331698.\n",
      "Step: 3420000. Mean Reward: 6.406853146853067. Std of Reward: 6.023323887751263.\n",
      "Step: 3430000. Mean Reward: 6.117397260273895. Std of Reward: 6.011139323412087.\n",
      "Step: 3440000. Mean Reward: 6.1872945205478675. Std of Reward: 5.993519149045005.\n",
      "Step: 3450000. Mean Reward: 5.913355048859861. Std of Reward: 6.120115773611518.\n",
      "Saved Model\n",
      "Step: 3460000. Mean Reward: 6.293172413793025. Std of Reward: 6.036587549777997.\n",
      "Step: 3470000. Mean Reward: 6.110890410958828. Std of Reward: 5.968000562501382.\n",
      "Step: 3480000. Mean Reward: 6.841854545454461. Std of Reward: 6.296122141064745.\n",
      "Step: 3490000. Mean Reward: 6.316288659793735. Std of Reward: 6.148249210672677.\n",
      "Step: 3500000. Mean Reward: 6.402456747404765. Std of Reward: 6.280001475466812.\n",
      "Saved Model\n",
      "Step: 3510000. Mean Reward: 6.144046822742398. Std of Reward: 6.165097140650714.\n",
      "Step: 3520000. Mean Reward: 5.911237458193904. Std of Reward: 5.9368856745076934.\n",
      "Step: 3530000. Mean Reward: 6.159453924914598. Std of Reward: 6.064821157667999.\n",
      "Step: 3540000. Mean Reward: 5.837236842105187. Std of Reward: 5.899319619262955.\n",
      "Step: 3550000. Mean Reward: 5.2437003058103295. Std of Reward: 5.636519362862124.\n",
      "Saved Model\n",
      "Step: 3560000. Mean Reward: 5.8828999999999265. Std of Reward: 6.136778844801166.\n",
      "Step: 3570000. Mean Reward: 6.43938566552893. Std of Reward: 6.247902771521617.\n",
      "Step: 3580000. Mean Reward: 5.57341935483864. Std of Reward: 5.807552861876987.\n",
      "Step: 3590000. Mean Reward: 5.930372881355855. Std of Reward: 5.712566895794292.\n",
      "Step: 3600000. Mean Reward: 6.7739999999999165. Std of Reward: 6.321234344363665.\n",
      "Saved Model\n",
      "Step: 3610000. Mean Reward: 6.6049275362318. Std of Reward: 6.044476187896891.\n",
      "Step: 3620000. Mean Reward: 5.774857142857072. Std of Reward: 6.137180850285804.\n",
      "Step: 3630000. Mean Reward: 6.747777777777693. Std of Reward: 6.222241129003463.\n",
      "Step: 3640000. Mean Reward: 6.633705035971138. Std of Reward: 6.094365567340957.\n",
      "Step: 3650000. Mean Reward: 6.819566787003526. Std of Reward: 6.346932272429678.\n",
      "Saved Model\n",
      "Step: 3660000. Mean Reward: 7.384334600760366. Std of Reward: 6.429835476360217.\n",
      "Step: 3670000. Mean Reward: 7.818461538461442. Std of Reward: 6.279204356424021.\n",
      "Step: 3680000. Mean Reward: 7.915599999999904. Std of Reward: 6.405956091013978.\n",
      "Step: 3690000. Mean Reward: 7.287915057914965. Std of Reward: 6.302321100314646.\n",
      "Step: 3700000. Mean Reward: 7.064962962962876. Std of Reward: 6.3459133976504285.\n",
      "Saved Model\n",
      "Step: 3710000. Mean Reward: 7.7009338521399835. Std of Reward: 6.390815143161874.\n",
      "Step: 3720000. Mean Reward: 7.0648880597014045. Std of Reward: 6.199564951227096.\n",
      "Step: 3730000. Mean Reward: 8.025469387755004. Std of Reward: 6.339885298331569.\n",
      "Step: 3740000. Mean Reward: 7.758914728682077. Std of Reward: 6.591427688841315.\n",
      "Step: 3750000. Mean Reward: 8.278429752066014. Std of Reward: 6.35908096799066.\n",
      "Saved Model\n",
      "Step: 3760000. Mean Reward: 7.792086614173135. Std of Reward: 6.564943461593128.\n",
      "Step: 3770000. Mean Reward: 7.866865079364982. Std of Reward: 6.223362672225486.\n",
      "Step: 3780000. Mean Reward: 7.778565737051699. Std of Reward: 6.4014134655739525.\n",
      "Step: 3790000. Mean Reward: 7.429615384615293. Std of Reward: 6.252052328124629.\n",
      "Step: 3800000. Mean Reward: 8.271291666666565. Std of Reward: 6.242832790349508.\n",
      "Saved Model\n",
      "Step: 3810000. Mean Reward: 8.177261410788281. Std of Reward: 6.2666197922372975.\n",
      "Step: 3820000. Mean Reward: 7.9218775510203105. Std of Reward: 6.225608186444744.\n",
      "Step: 3830000. Mean Reward: 7.822310756972015. Std of Reward: 6.216479916226512.\n",
      "Step: 3840000. Mean Reward: 7.716494023904288. Std of Reward: 6.303181551271557.\n",
      "Step: 3850000. Mean Reward: 8.412041666666566. Std of Reward: 6.537048996420043.\n",
      "Saved Model\n",
      "Step: 3860000. Mean Reward: 8.338436213991667. Std of Reward: 6.319040925093669.\n",
      "Step: 3870000. Mean Reward: 7.693921568627356. Std of Reward: 6.401708068158532.\n",
      "Step: 3880000. Mean Reward: 7.8138247011951245. Std of Reward: 6.451021824704612.\n",
      "Step: 3890000. Mean Reward: 8.385815899581488. Std of Reward: 6.310181457349984.\n",
      "Step: 3900000. Mean Reward: 7.924780876493927. Std of Reward: 6.3865930486227995.\n",
      "Saved Model\n",
      "Step: 3910000. Mean Reward: 8.084637096774095. Std of Reward: 6.478008694228025.\n",
      "Step: 3920000. Mean Reward: 8.18558232931717. Std of Reward: 6.385829989977157.\n",
      "Step: 3930000. Mean Reward: 7.697701612903131. Std of Reward: 6.338853816052775.\n",
      "Step: 3940000. Mean Reward: 7.585310077519288. Std of Reward: 6.431477407877373.\n",
      "Step: 3950000. Mean Reward: 8.053799999999901. Std of Reward: 6.472359535748851.\n",
      "Saved Model\n",
      "Step: 3960000. Mean Reward: 7.899722222222127. Std of Reward: 6.523155179379617.\n",
      "Step: 3970000. Mean Reward: 7.882470119521816. Std of Reward: 6.288281413990356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3980000. Mean Reward: 7.867119999999903. Std of Reward: 6.421149842948619.\n",
      "Step: 3990000. Mean Reward: 7.424671814671723. Std of Reward: 6.441655776226925.\n",
      "Step: 4000000. Mean Reward: 7.8441935483870004. Std of Reward: 6.251420754229324.\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-4000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-4000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-4000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mousegame2\\model-4000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
